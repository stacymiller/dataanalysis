\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[russian, english]{babel}
\usepackage[margin=1.5cm]{geometry}
\usepackage{minted}

\setlength{\parindent}{0cm}

\begin{document}

\section{Генерация тестовых данных}
<<echo=False>>=
import numpy as np
import random
from scipy import stats
from sklearn import linear_model
n = 5 # number of features
N = 100 # number of observations
np.set_printoptions(precision=4)
@
Сгенерируем тестовые данные. Положим число признаков $n=5$, общее число наблюдений --- $N=100$. Пусть большей частью набора будут нормально распределённые случайные вектора с заранее заданной ковариационной матрицей и математическими ожиданиями $(m_1, \ldots, m_n)$, выбросами --- две реализации нормально распределённого случайного вектора с такой же ковариационной матрицей и другим математическим ожиданием, две реализации равномерно распределённого на $[m_1-1;m_1+1]\times\cdots\times[m_n-1;m_n+1]$ случайного вектора и одна константа $\mathbf{0}\in \mathbb{R}^n$. Ковариационная матрица, местоположение выбросов и их конкретные значения генерируются для задачи случайным образом.
\[\sum_{i=1}^m\int_{-\infty}^x p_i \frac{1}{\sqrt{2\pi\sigma_i^2}}\mathsf{exp(}\frac{(x-\mu_i)^2}{2\sigma_i^2}\mathsf{)}\] 

<<wrap=False>>=
covariance = np.zeros((n,n))
for i in xrange(n):
  for j in xrange(n):
    covariance[i,j] = covariance[j, i] if covariance[j, i] != 0 else 2*random.random()-1
covariance = covariance.dot(np.matrix.transpose(covariance))
#
@
<<echo=False>>=
print "covariance:\n{cov}\n".format(cov=covariance)
@
<<>>=
outliers = random.sample(range(N), 5)
@
<<echo=False>>=
print "outliers:\n{cov}\n".format(cov=outliers)
@
<<>>=
outlier_mean = np.random.randint(1,10, size=n)
@
<<echo=False>>=
print "outlier_mean:\n{cov}\n".format(cov=outlier_mean)
@
<<wrap=False>>=
def random_vector_with_outliers(R, N=1):
  k = len(R[0]) # = n
  mean = np.random.uniform(-5,5, size=k)
  x = map(lambda i: mean + np.linalg.cholesky(R).dot(np.random.normal(size=k)), xrange(N))
  for (i, index) in enumerate(outliers):
    if i in [0, 1]:
      x[index] = outlier_mean + np.linalg.cholesky(R).dot(np.random.normal(size=k))
    elif i in [2, 3]:
      x[index] = mean + np.random.uniform(0, 1, size=k)
    else:
      x[index] = np.full((k, ), 0)
  return np.array(x)
X = random_vector_with_outliers(covariance, N)
@
<<echo=False>>=
np.savetxt("report2data.csv", X, delimiter=",")
@

\section{Проверка выборки на выбросы}

Положим $\bar{x} = \frac{1}{\#I}\sum_{x\in I} x$, где $x = (x_{i1}, \ldots, x_{ip})$ -- вектор, соответствующий одному наблюдению, $\hat{\Sigma}_I = \frac{1}{\#I-1}\sum_{x\in I}(x - \bar{x})(x - \bar{x})^T$. Тогда для расстояния Махаланобиса $D_I^2 = (y - \bar{x})\hat{\Sigma}_I^{-1}(y-\bar{x})^T$ (функции от набора наблюдений $I$ и наблюдения $y$, к этому набору не относящегося) нам будет известно распределение случайной величины $\frac{(N - n)N}{(N^2-1)n}D^2 \sim F\left(n, N-n\right)$.

\subsection{Полный набор данных}
Реализуем вычисление расстояния Махаланобиса от вектора до набора данных, из которого этот вектор исключён. Параметром метода будет являться порядковый номер исключаемого вектора (можно реализовать более общий вариант с передачей всего набора данных в метод, но в нашем случае, где расстояние Махаланобиса будет пересчитываться для каждого наблюдения из набора, разумнее хранить преподсчитанные данные для всего набора, нежели пересчитывать их каждый раз).

%Стоит заметить, что в нашем случае, где всегда $I = X\setminus\{x_j\}$, можно определить $$\bar{x} = \frac{1}{N}\sum_{i=1}^N x_i,$$ $$\hat{\Sigma}_j = \frac{1}{N-2}\sum_{i\in 1:N\setminus j}\left(x_i - \left(\bar{x} - x_j\right)\right)\left(x_i - \left(\bar{x} - x_j\right)\right)^T,$$ $$D_j^2 = \left(x_j - \left(\bar{x} - x_j\right)\right)\hat{\Sigma}_I^{-1}\left(x_j-\left(\bar{x} - x_j\right)\right)^T.$$

<<wrap=False>>=
xmean = np.mean(X, axis=0)
def mahalanobis(j):
  xnorm = (N*xmean - X[j])/(N-1) # mean of X with X[j] excluded
  sigma = np.matrix(
    np.sum( # np.outer(v1, v2) generates outer product of vectors v1 and v2
      map(lambda x: np.outer(x - xnorm, x - xnorm), np.delete(X, j, axis=0)), axis=0) / (N - 2))
  # matrix.get() returns this matrix inversed, i.e. 
  # sigma.dot(sigma.getI()) == np.eye((len(sigma), ))
  dmah = (X[j] - xmean).dot(sigma.getI()).dot(X[j] - xmean)
  return dmah
@
Проверим для каждого $j\in 1:N$ гипотезу $H_0: X[j] \text{ не является выбросом}$. Тестовая статистика $t = \frac{(N - n)N}{(N^2-1)n}D^2$ будет иметь в случае верности нулевой гипотезы распределение $F\left(n, N-n\right)$, <<идеальным значением>> будет $t=0$, следовательно, критическая область -- правый хвост распределения Фишера. Посчитаем граничное значение:
<<>>=
critical = stats.f.ppf(0.975, n, N-n)
@
<<echo=False>>=
print "critical value:\n{c}".format(c=critical)
@
Для каждого наблюдения будем принимать гипотезу о принадлежности исходному распределению, если $t_i < \mathsf{critical}$.
Результаты теста для каждого наблюдения:
<<echo=False>>=
for i in xrange(N):
  t = (N-n)*N*mahalanobis(i)/(n*(N*N-1))
  print t, "adopt" if t < critical else "reject {outlier}{x}".format(outlier=("outlier " if i in outliers else ""), x=X[i]), "outlier {x}".format(x=X[i]) if t < critical and i in outliers else ""
# bla
@
\subsection{<<Учебный>> набор данных}
Из-за того, что в выборку включаются не только <<правильные>> значения, но и другие выбросы, эффект сглаживается, и некоторые выбросы остаются включёнными в выборку. Попробуем теперь не включать в выборку выбросы вовсе, считать исходной группой только <<правильные>> данные.
<<wrap=False>>=
xmean_wo = np.mean(np.delete(X, outliers, axis=0), axis=0)
sigma_wo = np.matrix(
    np.sum( # np.outer(v1, v2) generates outer product of vectors v1 and v2
      map(lambda x: np.outer(x - xmean_wo, x - xmean_wo), 
          np.delete(X, outliers, axis=0)), axis=0) / (N - len(outliers) - 1))
sigma_wo_inv = sigma_wo.getI()
mahalanobis_without_outliers = lambda y: (y - xmean_wo).dot(sigma_wo_inv).dot(y - xmean_wo)
#
@
Результаты теста:
<<echo=False>>=
for i in xrange(N):
  t = (N-n)*N*mahalanobis_without_outliers(X[i])/(n*(N*N-1))
  print t, "adopt" if t < critical else "reject {outlier}{x}".format(outlier=("outlier " if i in outliers else ""), x=X[i]), "outlier {x}".format(x=X[i]) if t < critical and i in outliers else ""
@

<<Правильность>> исходных данных не оказывает сильного влияния на результат: критерий Махаланобиса отбрасывает результаты с соотношением между компонентами векторов, не похожим на таковое в большинстве наблюдений, но мало чувствителен к подозрительно маленькому разбросу.
\end{document}