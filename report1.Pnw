\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage{amsmath}
\usepackage[english, russian]{babel}
\usepackage[margin=1.5cm]{geometry}
\usepackage{minted}

\setlength{\parindent}{0cm}

\begin{document}

\section{Генерация тестовых данных}

<<>>=
import numpy as np
import random
from scipy import stats
from sklearn import linear_model


n = 5 # number of features
N = 100 # number of observations

np.set_printoptions(precision=4)
@
Будем искать регрессию по $N=100$ наблюдениям $n=5$ признаков. \\
Создадим ковариационную матрицу \texttt{covariance}, определяющую взаимозависимость признаков.
<<wrap=False>>=
covariance = np.zeros((n,n))
for i in xrange(n):
  for j in xrange(n):
    covariance[i,j] = covariance[j, i] if covariance[j, i] != 0 else 2*random.random()-1
  covariance = np.dot(covariance, np.matrix.transpose(covariance))
#blabla
@

<<echo=False, wrap=False>>=
print "covariance:\n{cov}\n".format(cov=covariance) # show covariance matrix
@

Определим операцию \texttt{random\_vector(R, N=1)}, которая возвращает вектор длины $N$ (по умолчанию равной 1), состоящий из реализаций нормально распределённого случайного вектора с ковариационной матрицей \texttt{R}. 
<<wrap=False>>=
def random_vector(R, N=1):
  k = len(R[0]) # get number of features from covariance matrix because relying on global variables is a bad idea
  x = np.empty((N, k)) # create empty array of given shape
  # fill this array with random observations of normal distribution with given covariance matrix R
  for i in xrange(N):
    s = np.random.normal(size=len(R[0])) 
    x[i] = np.dot(np.linalg.cholesky(R), s)
  return x
@

Сгенерируем выборку \texttt{X} и получим значения \texttt{Y} как линейную функцию от \texttt{X} со случайным шумом.  
<<>>=
# get random sample
X = random_vector(covariance, N)

beta = np.array([3,2,0,1,1]) # regression coefficients
sigma = 0.01 # noise variance

# get Y that we will try to predict by X
Y = np.add(np.dot(X, beta), np.random.normal(scale=sigma, size=N)) 
@

Таким образом, мы получили набор $\left\{\left\{x_{ji}\right\}_{i=1}^p\right\}_{j=1}^n$ и набор $\left\{y_i\right\}_{i=1}^n$, причём $$\label{eq:realregresstion}y_i = 3 \cdot x_{1i} + 2 \cdot x_{2i} + 0 \cdot x_{3i} + 1 \cdot x_{4i} + 1\cdot x_{5i} + \varepsilon_i, \varepsilon_i \sim N\left(0,1\right)$$.

\section{Стандартная линейная регрессия}
Объект \texttt{clf} после вызова функции \texttt{fit()} будет иметь всю информацию по поводу предсказания \texttt{Y} по \texttt{X}:
<<>>=
clf = linear_model.LinearRegression()
clf.fit(X, Y)
@
\texttt{LinearRegression()} подразумевает модель регрессии вида $y = \beta_0 + \beta_1 \cdot x_1 + \ldots + \beta_p \cdot x_p$, рассматривая которую, мы должны получить коэффициенты регрессии, близкие к $\beta_0 = 0, \beta_1 = 3, \beta_2 = 2, \beta_3 = 0, \beta_4 = 1, \beta_5 = 1$. 
<<echo=False, wrap=False>>=
print "estimated regression:"
print clf.intercept_, clf.coef_
print "real regression:"
print beta
@
\section{Подсчёт коэффициентов линейной регрессии с помощью матрицы вторых моментов}
Оценим коэффициенты линейной регрессии в модели $y_i = \alpha + \beta_1(x_{1i} - \bar{x_1}) + \ldots + \beta_p(x_{pi} - \bar{x_p})$ (\texttt{L} -- матрица вторых центральных моментов внутри $X$, \texttt{L0} -- вектор вторых центральных моментов между $X$ и $Y$, \texttt{mean\_x} = $\bar{X}$).
Научимся считать алгебраические дополнения:
<<>>=
def matrix_cofactor(matrix, row, col):
    nrows, ncols = matrix.shape
    minor = np.zeros([nrows-1, ncols-1])
    minor[:row,:col] = matrix[:row,:col]
    minor[row:,:col] = matrix[row+1:,:col]
    minor[:row,col:] = matrix[:row,col+1:]
    minor[row:,col:] = matrix[row+1:,col+1:]
    C = (-1)**(row+col) * np.linalg.det(minor)
    return C
@
Построим матрицу вторых моментов и оценки параметров по ней:
<<>>=
L = np.empty((n, n))
mean_x = np.mean(X, axis=0)
for i in xrange(n):
  for j in xrange(n):
    # map(f, X, ...) <=> sapply(X, f, ...)
    L[i][j] = np.average(
      map(lambda t: 
        (X[t][i] - mean_x[i])*(X[t][j] - mean_x[j]), xrange(N)))
L0 = np.empty((n, ))
mean_y = np.mean(Y)
L0 = map(lambda j: np.average(
  map(lambda t: 
    (Y[t] - mean_y)*(X[t][j] - mean_x[j]), xrange(N))), xrange(n))

LSM_expected_alpha = mean_y
LSM_expected_beta = map(lambda i: 
  np.sum(map(lambda j: 
    L0[j]*matrix_cofactor(L,i,j), xrange(n)
  ))/np.linalg.det(L), xrange(n))
@
Оценки вектора $(\beta_1, \ldots , \beta_5)$ должны оказаться похожими на изначально заданное \texttt{beta}:
<<echo=False, wrap=False>>=
print "regression estimates, calculated using cofactors:"
print np.array(LSM_expected_beta)
print "real regression:"
print beta
@
Параметр $\beta_0$, равный 0 в нашем случае, получается как $\beta_0 = \alpha - \beta_1\bar{x_1} + \ldots + \beta_p\bar{x_p} $
<<wrap=False>>=
beta_0 = LSM_expected_alpha - np.sum(map(lambda x,y: x*y, beta, mean_x))
print beta_0
@

\end{document}




